#!/usr/bin/env python3
"""
O-RedMind Ollama Integration
============================

IntÃ©gration native avec Ollama pour des modÃ¨les de langage
100% locaux et souverains selon le Manifeste OpenRed.

Auteur: SystÃ¨me OpenRed 2025
Licence: MIT - SouverainetÃ© NumÃ©rique Totale
"""

import requests
import json
import time
import logging
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any, Iterator
from dataclasses import dataclass
import threading
import os
import platform

logger = logging.getLogger(__name__)

@dataclass
class OllamaModel:
    """ModÃ¨le Ollama disponible"""
    name: str
    size: str
    description: str
    capabilities: List[str]
    recommended_ram: str
    language_support: List[str]

@dataclass
class ChatMessage:
    """Message de chat pour Ollama"""
    role: str  # 'user', 'assistant', 'system'
    content: str
    timestamp: float

class OllamaIntegration:
    """IntÃ©gration native avec Ollama pour O-RedMind"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.current_model = None
        self.available_models = []
        self.is_connected = False
        self.conversation_history = []
        
        # ModÃ¨les recommandÃ©s pour O-RedMind
        self.recommended_models = {
            'phi3': OllamaModel(
                name='phi3',
                size='2.3GB',
                description='ModÃ¨le Microsoft Phi-3 optimisÃ©, excellent pour conversations',
                capabilities=['chat', 'raisonnement', 'code'],
                recommended_ram='4GB',
                language_support=['franÃ§ais', 'anglais', 'code']
            ),
            'llama3.2': OllamaModel(
                name='llama3.2',
                size='2.0GB',
                description='Meta Llama 3.2 compact et performant',
                capabilities=['chat', 'crÃ©ativitÃ©', 'analyse'],
                recommended_ram='4GB',
                language_support=['franÃ§ais', 'anglais', 'multilingue']
            ),
            'mistral': OllamaModel(
                name='mistral',
                size='4.1GB',
                description='Mistral 7B, excellent Ã©quilibre performance/taille',
                capabilities=['chat', 'raisonnement', 'crÃ©ativitÃ©', 'code'],
                recommended_ram='8GB',
                language_support=['franÃ§ais', 'anglais', 'code', 'multilingue']
            ),
            'codellama': OllamaModel(
                name='codellama',
                size='3.8GB',
                description='SpÃ©cialisÃ© dans la programmation et le code',
                capabilities=['code', 'debugging', 'explication'],
                recommended_ram='8GB',
                language_support=['python', 'javascript', 'java', 'c++', 'rust']
            ),
            'qwen2': OllamaModel(
                name='qwen2',
                size='4.4GB',
                description='Alibaba Qwen2, multilingue et performant',
                capabilities=['chat', 'multilingue', 'raisonnement'],
                recommended_ram='8GB',
                language_support=['franÃ§ais', 'anglais', 'chinois', 'multilingue']
            )
        }
        
        self._check_ollama_status()
    
    def _check_ollama_status(self) -> bool:
        """VÃ©rifie si Ollama est disponible"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            self.is_connected = response.status_code == 200
            
            if self.is_connected:
                self.available_models = self._parse_available_models(response.json())
                logger.info(f"âœ… Ollama connectÃ© - {len(self.available_models)} modÃ¨les disponibles")
            else:
                logger.warning("âš ï¸  Ollama non accessible")
                
            return self.is_connected
            
        except requests.exceptions.RequestException as e:
            self.is_connected = False
            logger.warning(f"âš ï¸  Ollama non disponible: {e}")
            return False
    
    def _parse_available_models(self, response_data: Dict) -> List[str]:
        """Parse les modÃ¨les disponibles depuis la rÃ©ponse Ollama"""
        models = []
        if 'models' in response_data:
            for model in response_data['models']:
                models.append(model['name'])
        return models
    
    def is_ollama_installed(self) -> bool:
        """VÃ©rifie si Ollama est installÃ© sur le systÃ¨me"""
        return shutil.which('ollama') is not None
    
    def install_ollama_instructions(self) -> Dict[str, str]:
        """Retourne les instructions d'installation d'Ollama"""
        system = platform.system().lower()
        
        instructions = {
            'windows': 'TÃ©lÃ©chargez Ollama depuis https://ollama.ai et exÃ©cutez le programme d\'installation',
            'darwin': 'TÃ©lÃ©chargez Ollama depuis https://ollama.ai ou utilisez: brew install ollama',
            'linux': 'curl -fsSL https://ollama.ai/install.sh | sh'
        }
        
        return {
            'system': system,
            'instruction': instructions.get(system, instructions['linux']),
            'url': 'https://ollama.ai',
            'recommended_models': list(self.recommended_models.keys())
        }
    
    def pull_model(self, model_name: str) -> Iterator[Dict[str, Any]]:
        """TÃ©lÃ©charge un modÃ¨le Ollama avec suivi de progression"""
        if not self.is_connected:
            yield {'error': 'Ollama non connectÃ©'}
            return
        
        try:
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={'name': model_name},
                stream=True,
                timeout=300
            )
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        yield data
                    except json.JSONDecodeError:
                        continue
                        
        except requests.exceptions.RequestException as e:
            yield {'error': f'Erreur tÃ©lÃ©chargement: {str(e)}'}
    
    def list_models(self) -> List[Dict[str, Any]]:
        """Liste tous les modÃ¨les disponibles"""
        if not self.is_connected:
            return []
        
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = []
                
                for model in data.get('models', []):
                    model_info = {
                        'name': model['name'],
                        'size': model.get('size', 0),
                        'modified': model.get('modified', ''),
                        'is_recommended': any(rec in model['name'] for rec in self.recommended_models.keys())
                    }
                    
                    # Ajout des infos recommandÃ©es si disponibles
                    for rec_name, rec_model in self.recommended_models.items():
                        if rec_name in model['name']:
                            model_info.update({
                                'description': rec_model.description,
                                'capabilities': rec_model.capabilities,
                                'recommended_ram': rec_model.recommended_ram,
                                'language_support': rec_model.language_support
                            })
                            break
                    
                    models.append(model_info)
                
                return models
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur liste modÃ¨les: {e}")
            return []
    
    def set_model(self, model_name: str) -> bool:
        """DÃ©finit le modÃ¨le actuel"""
        if model_name in self.available_models:
            self.current_model = model_name
            logger.info(f"ğŸ“ ModÃ¨le sÃ©lectionnÃ©: {model_name}")
            return True
        else:
            logger.warning(f"âš ï¸  ModÃ¨le non disponible: {model_name}")
            return False
    
    def chat(self, message: str, system_prompt: Optional[str] = None, 
             temperature: float = 0.7, max_tokens: int = 2000) -> Iterator[str]:
        """Chat avec le modÃ¨le Ollama en streaming"""
        if not self.is_connected or not self.current_model:
            yield "âŒ Ollama non connectÃ© ou modÃ¨le non sÃ©lectionnÃ©"
            return
        
        # PrÃ©paration des messages
        messages = []
        
        # Ajout du prompt systÃ¨me si fourni
        if system_prompt:
            messages.append({
                'role': 'system',
                'content': system_prompt
            })
        
        # Ajout de l'historique rÃ©cent (5 derniers messages)
        recent_history = self.conversation_history[-10:]  # 5 paires user/assistant
        messages.extend([
            {'role': msg.role, 'content': msg.content} 
            for msg in recent_history
        ])
        
        # Ajout du message actuel
        messages.append({
            'role': 'user',
            'content': message
        })
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    'model': self.current_model,
                    'messages': messages,
                    'stream': True,
                    'options': {
                        'temperature': temperature,
                        'num_predict': max_tokens
                    }
                },
                stream=True,
                timeout=60
            )
            
            full_response = ""
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        
                        if 'message' in data and 'content' in data['message']:
                            content = data['message']['content']
                            full_response += content
                            yield content
                        
                        if data.get('done', False):
                            # Sauvegarde dans l'historique
                            self.conversation_history.append(
                                ChatMessage('user', message, time.time())
                            )
                            self.conversation_history.append(
                                ChatMessage('assistant', full_response, time.time())
                            )
                            
                            # Limite l'historique
                            if len(self.conversation_history) > 20:
                                self.conversation_history = self.conversation_history[-20:]
                            
                            break
                            
                    except json.JSONDecodeError:
                        continue
                        
        except requests.exceptions.RequestException as e:
            yield f"âŒ Erreur communication: {str(e)}"
    
    def simple_chat(self, message: str, system_prompt: Optional[str] = None) -> str:
        """Chat simple non-streamÃ©"""
        response_parts = []
        for chunk in self.chat(message, system_prompt):
            response_parts.append(chunk)
        return ''.join(response_parts)
    
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """RÃ©cupÃ¨re les informations dÃ©taillÃ©es d'un modÃ¨le"""
        if not self.is_connected:
            return None
        
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={'name': model_name}
            )
            
            if response.status_code == 200:
                return response.json()
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur info modÃ¨le: {e}")
            return None
    
    def delete_model(self, model_name: str) -> bool:
        """Supprime un modÃ¨le"""
        if not self.is_connected:
            return False
        
        try:
            response = requests.delete(
                f"{self.base_url}/api/delete",
                json={'name': model_name}
            )
            
            if response.status_code == 200:
                self.available_models = [m for m in self.available_models if m != model_name]
                if self.current_model == model_name:
                    self.current_model = None
                return True
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur suppression modÃ¨le: {e}")
            return False
    
    def get_recommended_model(self, capabilities: List[str] = None) -> Optional[str]:
        """Recommande un modÃ¨le selon les capacitÃ©s demandÃ©es"""
        if not capabilities:
            capabilities = ['chat']
        
        # Recherche du meilleur modÃ¨le disponible
        for model_name, model_info in self.recommended_models.items():
            if model_name in self.available_models:
                if any(cap in model_info.capabilities for cap in capabilities):
                    return model_name
        
        # Fallback sur le premier modÃ¨le disponible
        return self.available_models[0] if self.available_models else None
    
    def clear_conversation(self):
        """Efface l'historique de conversation"""
        self.conversation_history.clear()
        logger.info("ğŸ—‘ï¸  Historique de conversation effacÃ©")
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le status complet d'Ollama"""
        return {
            'connected': self.is_connected,
            'base_url': self.base_url,
            'current_model': self.current_model,
            'available_models': self.available_models,
            'recommended_models': list(self.recommended_models.keys()),
            'conversation_length': len(self.conversation_history),
            'ollama_installed': self.is_ollama_installed()
        }

def main():
    """Test de l'intÃ©gration Ollama"""
    print("ğŸ¦™ O-RedMind Ollama Integration")
    print("=" * 45)
    
    ollama = OllamaIntegration()
    
    # Status
    status = ollama.get_status()
    print(f"ğŸ“¡ Connexion: {'âœ… OK' if status['connected'] else 'âŒ NOK'}")
    print(f"ğŸ¦™ Ollama installÃ©: {'âœ… Oui' if status['ollama_installed'] else 'âŒ Non'}")
    
    if not status['connected']:
        if not status['ollama_installed']:
            print("\nğŸ’¡ Installation d'Ollama requise:")
            instructions = ollama.install_ollama_instructions()
            print(f"   SystÃ¨me: {instructions['system']}")
            print(f"   Commande: {instructions['instruction']}")
            print(f"   URL: {instructions['url']}")
        else:
            print("\nâš ï¸  Ollama installÃ© mais non dÃ©marrÃ©. Lancez: ollama serve")
        return
    
    # Liste des modÃ¨les
    models = ollama.list_models()
    print(f"\nğŸ“š ModÃ¨les disponibles: {len(models)}")
    
    for model in models:
        status_icon = "â­" if model.get('is_recommended') else "ğŸ“¦"
        print(f"   {status_icon} {model['name']}")
        if 'description' in model:
            print(f"     {model['description']}")
    
    if not models:
        print("\nğŸ’¡ Aucun modÃ¨le installÃ©. ModÃ¨les recommandÃ©s:")
        for name, info in ollama.recommended_models.items():
            print(f"   â­ {name} ({info.size}) - {info.description}")
        print("\nPour installer un modÃ¨le: ollama pull phi3")
        return
    
    # Test de chat si modÃ¨le disponible
    if models:
        recommended = ollama.get_recommended_model(['chat'])
        if recommended:
            ollama.set_model(recommended)
            print(f"\nğŸ¤– Test avec {recommended}:")
            
            response = ollama.simple_chat(
                "Bonjour ! PrÃ©sente-toi en franÃ§ais en 2 phrases.",
                "Tu es O-RedMind, l'IA personnelle respectueuse de la souverainetÃ© numÃ©rique."
            )
            print(f"   {response}")
    
    print("\nâœ… IntÃ©gration Ollama prÃªte pour O-RedMind !")

if __name__ == "__main__":
    main()