#!/usr/bin/env python3
"""
O-RedMind Setup et Configuration
===============================

Script d'installation et configuration compl√®te pour O-RedMind
avec int√©gration Ollama optimis√©e.

Auteur: Syst√®me OpenRed 2025
Licence: MIT - Souverainet√© Num√©rique Totale
"""

import os
import sys
import json
import time
import subprocess
import platform
from pathlib import Path
from typing import Dict, List, Optional

def print_header():
    """Affiche l'en-t√™te de setup"""
    print("üß† O-RedMind Setup & Configuration")
    print("=" * 50)
    print("üîí 100% Local ‚Ä¢ ü¶ô Powered by Ollama ‚Ä¢ üéØ Souverainet√© Totale")
    print()

def check_ollama_status():
    """V√©rifie le status d'Ollama"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return True, models
    except:
        pass
    return False, []

def get_recommended_models():
    """Retourne les mod√®les recommand√©s pour O-RedMind"""
    return {
        'phi3': {
            'size': '2.3GB',
            'description': 'Mod√®le Microsoft Phi-3, excellent pour conversations',
            'use_case': 'Chat g√©n√©ral, raisonnement',
            'ram_required': '4GB'
        },
        'llama3.2': {
            'size': '2.0GB', 
            'description': 'Meta Llama 3.2 compact et performant',
            'use_case': 'Chat, cr√©ativit√©, analyse',
            'ram_required': '4GB'
        },
        'mistral': {
            'size': '4.1GB',
            'description': 'Mistral 7B, excellent √©quilibre performance/taille',
            'use_case': 'Chat avanc√©, raisonnement complexe',
            'ram_required': '8GB'
        },
        'codellama': {
            'size': '3.8GB',
            'description': 'Sp√©cialis√© programmation et code',
            'use_case': 'D√©veloppement, debugging, explication code',
            'ram_required': '8GB'
        }
    }

def suggest_models_for_system():
    """Sugg√®re des mod√®les selon les ressources syst√®me"""
    import psutil
    
    # M√©moire RAM disponible (en GB)
    ram_gb = psutil.virtual_memory().total / (1024**3)
    
    print(f"üíæ RAM syst√®me d√©tect√©e: {ram_gb:.1f}GB")
    
    if ram_gb >= 16:
        return ['mistral', 'codellama', 'phi3']
    elif ram_gb >= 8:
        return ['phi3', 'llama3.2', 'mistral']
    else:
        return ['phi3', 'llama3.2']

def install_model(model_name: str):
    """Installe un mod√®le Ollama"""
    print(f"ü¶ô Installation de {model_name}...")
    
    try:
        process = subprocess.Popen(
            ['ollama', 'pull', model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(f"   {output.strip()}")
        
        if process.returncode == 0:
            print(f"   ‚úÖ {model_name} install√© avec succ√®s")
            return True
        else:
            print(f"   ‚ùå Erreur installation {model_name}")
            return False
            
    except FileNotFoundError:
        print("   ‚ùå Ollama non trouv√© dans le PATH")
        return False
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False

def create_oredmind_config():
    """Cr√©e la configuration O-RedMind"""
    config_dir = Path.home() / ".openred" / "config"
    config_dir.mkdir(exist_ok=True, parents=True)
    
    config = {
        'oredmind': {
            'version': '1.0.0',
            'mode': 'ollama_enhanced',
            'ollama': {
                'enabled': True,
                'base_url': 'http://localhost:11434',
                'preferred_model': None,
                'fallback_enabled': True
            },
            'privacy': {
                'learning_mode': 'private_only',
                'data_retention_days': 365,
                'consent_required': True
            },
            'profiles': {
                'default': 'Professionnel',
                'adaptation_enabled': True
            }
        }
    }
    
    config_file = config_dir / "oredmind.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Configuration sauv√©e: {config_file}")
    return config

def main():
    """Setup principal"""
    print_header()
    
    # 1. V√©rification Ollama
    print("1Ô∏è‚É£ V√©rification d'Ollama...")
    ollama_connected, current_models = check_ollama_status()
    
    if not ollama_connected:
        print("‚ùå Ollama non connect√©")
        print("\nüí° Instructions:")
        print("   1. V√©rifiez qu'Ollama est install√©: https://ollama.ai")
        print("   2. D√©marrez Ollama: ollama serve")
        print("   3. Relancez ce script")
        return False
    
    print("‚úÖ Ollama connect√©")
    print(f"üìö Mod√®les actuels: {len(current_models)}")
    
    for model in current_models:
        print(f"   ‚Ä¢ {model['name']}")
    
    # 2. Suggestion de mod√®les
    print("\n2Ô∏è‚É£ Recommandations de mod√®les...")
    
    try:
        suggested = suggest_models_for_system()
        recommended = get_recommended_models()
        
        print("üéØ Mod√®les recommand√©s pour votre syst√®me:")
        for model_name in suggested:
            if model_name in recommended:
                info = recommended[model_name]
                print(f"   ‚≠ê {model_name} ({info['size']}) - {info['description']}")
                print(f"      Usage: {info['use_case']}")
        
        # V√©rification des mod√®les d√©j√† install√©s
        current_model_names = [m['name'] for m in current_models]
        missing_models = []
        
        for model_name in suggested:
            model_found = any(model_name in name for name in current_model_names)
            if not model_found:
                missing_models.append(model_name)
        
        if missing_models:
            print(f"\nüì• Mod√®les manquants recommand√©s: {', '.join(missing_models)}")
            
            install_choice = input(f"\nü§î Installer les mod√®les manquants ? (o/N): ").lower()
            
            if install_choice in ['o', 'oui', 'y', 'yes']:
                print("\nüì¶ Installation en cours...")
                
                for model_name in missing_models:
                    if install_model(model_name):
                        time.sleep(1)  # Pause entre installations
                    else:
                        print(f"‚ö†Ô∏è  Installation de {model_name} √©chou√©e")
            else:
                print("‚è≠Ô∏è  Installation des mod√®les ignor√©e")
        else:
            print("‚úÖ Tous les mod√®les recommand√©s sont disponibles")
    
    except ImportError:
        print("‚ö†Ô∏è  psutil non disponible, suggestion automatique impossible")
    
    # 3. Configuration O-RedMind
    print("\n3Ô∏è‚É£ Configuration O-RedMind...")
    config = create_oredmind_config()
    
    # D√©tection du meilleur mod√®le disponible
    ollama_connected, updated_models = check_ollama_status()
    if updated_models:
        # Priorit√© aux mod√®les recommand√©s
        preferred_model = None
        for model_name in ['phi3', 'mistral', 'llama3.2']:
            for model in updated_models:
                if model_name in model['name']:
                    preferred_model = model['name']
                    break
            if preferred_model:
                break
        
        if not preferred_model:
            preferred_model = updated_models[0]['name']
        
        config['oredmind']['ollama']['preferred_model'] = preferred_model
        print(f"üéØ Mod√®le pr√©f√©r√© configur√©: {preferred_model}")
    
    # 4. Test de fonctionnement
    print("\n4Ô∏è‚É£ Test de fonctionnement...")
    
    try:
        from ollama_integration import OllamaIntegration
        
        ollama = OllamaIntegration()
        
        if ollama.is_connected:
            print("‚úÖ Int√©gration Ollama fonctionnelle")
            
            # Test avec le mod√®le configur√©
            recommended = ollama.get_recommended_model(['chat'])
            if recommended:
                ollama.set_model(recommended)
                
                print(f"üß™ Test avec {recommended}...")
                test_response = ollama.simple_chat(
                    "Dis bonjour en fran√ßais en une phrase.",
                    "Tu es O-RedMind, r√©ponds de mani√®re concise."
                )
                
                if test_response and not test_response.startswith('‚ùå'):
                    print(f"   ‚úÖ Test r√©ussi: {test_response[:50]}...")
                else:
                    print("   ‚ö†Ô∏è  Test de r√©ponse √©chou√©")
            else:
                print("   ‚ö†Ô∏è  Aucun mod√®le recommand√© disponible")
        else:
            print("‚ùå Int√©gration Ollama non fonctionnelle")
    
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")
    
    # 5. Instructions finales
    print("\nüéâ Setup O-RedMind termin√© !")
    print("\nüöÄ Prochaines √©tapes:")
    print("   1. Lancez O-RedMind: python interface_web.py")
    print("   2. Ouvrez: http://localhost:5000")
    print("   3. Commencez √† chatter avec votre IA souveraine !")
    
    print("\nüí° Conseils:")
    print("   ‚Ä¢ O-RedMind fonctionne 100% en local")
    print("   ‚Ä¢ Vos donn√©es restent sur votre machine")
    print("   ‚Ä¢ Aucune connexion internet requise apr√®s setup")
    print("   ‚Ä¢ Adaptez votre profil dans les param√®tres")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("\n‚úÖ O-RedMind est pr√™t √† r√©volutionner votre IA personnelle !")
        else:
            print("\n‚ùå Setup incomplet, consultez les instructions ci-dessus")
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è  Setup interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        print("üìû Consultez la documentation ou cr√©ez une issue GitHub")